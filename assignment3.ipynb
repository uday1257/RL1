{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpSQIE3nxyZCG/MNVMw762",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uday1257/RL1/blob/main/assignment3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh4OJ_YkHYA5",
        "outputId": "0f3aeb83-a08f-4118-90eb-c79560815b62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating baseline policy (stick on 20+, else hit)...\n",
            "Baseline policy average return (approx): -0.3421\n",
            "Learning with MC control (ε-greedy)... this may take a bit for many episodes.\n",
            "Final epsilon after training: 0.0500\n",
            "Greedy policy (from learned Q) average return (approx): -0.0532\n",
            "Q(20, 10, False) = [ 0.42193565 -0.84453782]\n",
            "Q(13, 2, False) = [-0.28460156 -0.44642857]\n",
            "Q(12, 6, True) = [-0.14285714  0.03191489]\n",
            "Q(18, 9, True) = [-0.18631179 -0.26666667]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "mc_blackjack.py\n",
        "\n",
        "Requires:\n",
        "    pip install gymnasium\n",
        "    # (Blackjack is included in gymnasium's toy_text environments)\n",
        "\n",
        "Run:\n",
        "    python mc_blackjack.py\n",
        "\"\"\"\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import Dict, Tuple, List, Callable\n",
        "\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def make_epsilon_greedy_policy(Q: Dict, nA: int, epsilon: float) -> Callable:\n",
        "    \"\"\"Return a policy function that takes state and returns action probabilities.\"\"\"\n",
        "    def policy_fn(state):\n",
        "        # Ensure state exists in Q\n",
        "        _ = Q[state]  # triggers defaultdict to create if missing\n",
        "        probs = np.ones(nA, dtype=float) * (epsilon / nA)\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "        return probs\n",
        "    return policy_fn\n",
        "\n",
        "\n",
        "def generate_episode(env, policy_fn: Callable) -> List[Tuple[Tuple, int, float]]:\n",
        "    \"\"\"Generate an episode: returns list of (state, action, reward). Uses policy as action-prob function.\"\"\"\n",
        "    episode: List[Tuple[Tuple, int, float]] = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        probs = policy_fn(state)\n",
        "        action = int(np.random.choice(len(probs), p=probs))\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "    return episode\n",
        "\n",
        "\n",
        "# ---------- First-Visit MC Policy Evaluation ----------\n",
        "def first_visit_mc_policy_evaluation(env, policy_fn: Callable, gamma: float = 1.0, num_episodes: int = 10000):\n",
        "    \"\"\"\n",
        "    Estimate V(s) for the given policy (state-values) with first-visit MC.\n",
        "\n",
        "    Args:\n",
        "        env: gymnasium environment\n",
        "        policy_fn: function mapping state -> action probabilities\n",
        "        gamma: discount factor\n",
        "        num_episodes: number of episodes to sample\n",
        "\n",
        "    Returns:\n",
        "        V: dict mapping state -> estimated value\n",
        "    \"\"\"\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        episode = generate_episode(env, policy_fn)\n",
        "        states_in_episode = set([x[0] for x in episode])\n",
        "        for state in states_in_episode:\n",
        "            first_idx = next(i for i, x in enumerate(episode) if x[0] == state)\n",
        "            G = 0.0\n",
        "            power = 0\n",
        "            for (_, _, r) in episode[first_idx:]:\n",
        "                G += (gamma ** power) * r\n",
        "                power += 1\n",
        "            returns_sum[state] += G\n",
        "            returns_count[state] += 1.0\n",
        "            V[state] = returns_sum[state] / returns_count[state]\n",
        "\n",
        "    return V\n",
        "\n",
        "\n",
        "# ---------- On-Policy MC Control with ε-greedy ----------\n",
        "def mc_control_epsilon_greedy(\n",
        "    env,\n",
        "    num_episodes: int = 500000,\n",
        "    gamma: float = 1.0,\n",
        "    epsilon_start: float = 1.0,\n",
        "    epsilon_min: float = 0.05,\n",
        "    epsilon_decay: float = 0.9995,\n",
        "):\n",
        "    \"\"\"\n",
        "    Learn optimal Q and a corresponding ε-greedy policy via on-policy MC control (first-visit).\n",
        "\n",
        "    Returns:\n",
        "        Q: action-value function (defaultdict of state -> np.array(nA))\n",
        "        policy_fn: ε-greedy policy derived from Q (with current epsilon)\n",
        "        epsilon: final epsilon used\n",
        "    \"\"\"\n",
        "    nA = env.action_space.n\n",
        "    Q = defaultdict(lambda: np.zeros(nA, dtype=float))\n",
        "    returns_sum = defaultdict(float)          # keyed by (state, action)\n",
        "    returns_count = defaultdict(float)        # keyed by (state, action)\n",
        "\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    def policy_for_episode(state):\n",
        "        # policy used while generating episodes (current ε)\n",
        "        probs = np.ones(nA, dtype=float) * (epsilon / nA)\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "        return probs\n",
        "\n",
        "    for ep in range(1, num_episodes + 1):\n",
        "        episode = generate_episode(env, policy_for_episode)\n",
        "\n",
        "        # Track first-visit indices for (s,a)\n",
        "        sa_first_visit = {}\n",
        "        for idx, (s, a, _) in enumerate(episode):\n",
        "            if (s, a) not in sa_first_visit:\n",
        "                sa_first_visit[(s, a)] = idx\n",
        "\n",
        "        # For each first-visited (s,a), compute return and update\n",
        "        for (s, a), first_idx in sa_first_visit.items():\n",
        "            G = 0.0\n",
        "            power = 0\n",
        "            for (_, _, r) in episode[first_idx:]:\n",
        "                G += (gamma ** power) * r\n",
        "                power += 1\n",
        "            returns_sum[(s, a)] += G\n",
        "            returns_count[(s, a)] += 1.0\n",
        "            Q[s][a] = returns_sum[(s, a)] / returns_count[(s, a)]\n",
        "\n",
        "        # Decay epsilon\n",
        "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
        "\n",
        "    # Return a policy function tied to the learned Q with the final epsilon\n",
        "    learned_policy = make_epsilon_greedy_policy(Q, env.action_space.n, epsilon)\n",
        "    return Q, learned_policy, epsilon\n",
        "\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def greedy_policy_from_Q(Q: Dict, nA: int) -> Callable:\n",
        "    \"\"\"Deterministic greedy policy from Q.\"\"\"\n",
        "    def policy_fn(state):\n",
        "        _ = Q[state]  # ensure exists\n",
        "        probs = np.zeros(nA, dtype=float)\n",
        "        best_a = int(np.argmax(Q[state]))\n",
        "        probs[best_a] = 1.0\n",
        "        return probs\n",
        "    return policy_fn\n",
        "\n",
        "\n",
        "def evaluate_policy_avg_return(env, policy_fn: Callable, episodes: int = 10000) -> float:\n",
        "    \"\"\"Estimate average return by running the given policy for a number of episodes.\"\"\"\n",
        "    total = 0.0\n",
        "    for _ in range(episodes):\n",
        "        ep = generate_episode(env, policy_fn)\n",
        "        total += sum(r for (_, _, r) in ep)\n",
        "    return total / episodes\n",
        "\n",
        "\n",
        "# ---------- Main ----------\n",
        "if __name__ == \"__main__\":\n",
        "    # Create Blackjack env (Gymnasium)\n",
        "    # Action space: 0 = Stick, 1 = Hit\n",
        "    env = gym.make(\"Blackjack-v1\")  # uses toy_text Blackjack\n",
        "    nA = env.action_space.n\n",
        "\n",
        "    # (A) Example: Evaluate a simple baseline policy (stick on 20+ else hit)\n",
        "    def baseline_policy(state):\n",
        "        player_sum, dealer_card, usable_ace = state\n",
        "        action = 0 if player_sum >= 20 else 1\n",
        "        probs = np.zeros(nA, dtype=float)\n",
        "        probs[action] = 1.0\n",
        "        return probs\n",
        "\n",
        "    print(\"Evaluating baseline policy (stick on 20+, else hit)...\")\n",
        "    V_baseline = first_visit_mc_policy_evaluation(env, baseline_policy, gamma=1.0, num_episodes=20000)\n",
        "    avg_return_baseline = evaluate_policy_avg_return(env, baseline_policy, episodes=10000)\n",
        "    print(f\"Baseline policy average return (approx): {avg_return_baseline:.4f}\")\n",
        "\n",
        "    # (B) Learn via MC control ε-greedy\n",
        "    print(\"Learning with MC control (ε-greedy)... this may take a bit for many episodes.\")\n",
        "    Q, learned_eps_policy, final_eps = mc_control_epsilon_greedy(\n",
        "        env,\n",
        "        num_episodes=200000,   # increase for stronger results\n",
        "        gamma=1.0,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_min=0.05,\n",
        "        epsilon_decay=0.9995,\n",
        "    )\n",
        "    greedy_pi = greedy_policy_from_Q(Q, nA)\n",
        "    avg_return_greedy = evaluate_policy_avg_return(env, greedy_pi, episodes=20000)\n",
        "\n",
        "    print(f\"Final epsilon after training: {final_eps:.4f}\")\n",
        "    print(f\"Greedy policy (from learned Q) average return (approx): {avg_return_greedy:.4f}\")\n",
        "\n",
        "    # Example: show a few Q-values\n",
        "    sample_states = [\n",
        "        (20, 10, False),  # player 20 vs dealer 10, no usable ace\n",
        "        (13, 2, False),\n",
        "        (12, 6, True),\n",
        "        (18, 9, True),\n",
        "    ]\n",
        "    for s in sample_states:\n",
        "        _ = Q[s]  # ensure exists\n",
        "        print(f\"Q{str(s)} = {Q[s]}\")"
      ]
    }
  ]
}